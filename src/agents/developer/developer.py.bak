"""
Module: agents/developer/developer.py

WHY: Main orchestrator for developer agent using composition pattern.
RESPONSIBILITY: Coordinate all developer workflows (TDD, quality-driven) using specialized components.
PATTERNS: Composition Pattern, Strategy Pattern, Facade Pattern.

This module:
- Composes FileManager, RAGIntegration, LLMClientWrapper, TDDPhases, ReportGenerator
- Provides public execute() method for backward compatibility
- Orchestrates workflow selection (TDD vs quality-driven)
- Delegates to specialized components (no God object!)

REPLACES: standalone_developer_agent.py (2,792 lines ‚Üí ~350 lines orchestration)
"""

import os
from pathlib import Path
from typing import Dict, List, Optional
from artemis_stage_interface import LoggerInterface
from debug_mixin import DebugMixin
from llm_client import create_llm_client
from artemis_exceptions import (
    LLMClientError,
    DeveloperExecutionError,
    create_wrapped_exception
)

# Import extracted modules
from agents.developer.file_manager import FileManager
from agents.developer.test_runner_wrapper import DeveloperTestRunner
from agents.developer.report_generator import ReportGenerator
from agents.developer.rag_integration import RAGIntegration
from agents.developer.llm_client_wrapper import LLMClientWrapper
from agents.developer.tdd_phases import TDDPhases
from agents.developer.models import WorkflowType, WorkflowContext

# Import optional dependencies
try:
    from prompt_manager import PromptManager
    PROMPT_MANAGER_AVAILABLE = True
except ImportError:
    PROMPT_MANAGER_AVAILABLE = False

try:
    from ai_query_service import create_ai_query_service
    AI_QUERY_SERVICE_AVAILABLE = True
except ImportError:
    AI_QUERY_SERVICE_AVAILABLE = False

try:
    from retry_coordinator import RetryCoordinator
    RETRY_COORDINATOR_AVAILABLE = True
except ImportError:
    RETRY_COORDINATOR_AVAILABLE = False


class Developer(DebugMixin):
    """
    Modular developer agent using composition pattern

    WHY: Replaces monolithic StandaloneDeveloperAgent with clean component composition
    PATTERNS: Composition, Strategy, Facade
    """

    def __init__(
        self,
        developer_name: str,
        developer_type: str,
        llm_provider: str = "openai",
        llm_model: Optional[str] = None,
        logger: Optional[LoggerInterface] = None,
        rag_agent=None,
        ai_service: Optional['AIQueryService'] = None
    ):
        """
        Initialize developer with composed components

        Args:
            developer_name: "developer-a" or "developer-b"
            developer_type: "conservative" or "aggressive"
            llm_provider: "openai" or "anthropic"
            llm_model: Specific model (optional, uses default)
            logger: Logger implementation
            rag_agent: RAG agent for prompt retrieval (optional)
            ai_service: AI Query Service for KG-First queries (optional)
        """
        DebugMixin.__init__(self, component_name="developer")

        # Store developer identity
        self.developer_name = developer_name
        self.developer_type = developer_type
        self.llm_provider = llm_provider
        self.llm_model = llm_model
        self.logger = logger

        self.debug_log("Developer initialized",
                      name=developer_name, type=developer_type, provider=llm_provider)

        # Create LLM client
        self.llm_client = self._create_llm_client()

        # Initialize optional dependencies
        self.prompt_manager = self._initialize_prompt_manager(rag_agent)
        self.ai_service = self._initialize_ai_service(ai_service, rag_agent)
        self.retry_coordinator = self._initialize_retry_coordinator()

        # Compose specialized components
        self.file_manager = FileManager(logger=logger)
        self.test_runner = DeveloperTestRunner(logger=logger)
        self.report_generator = ReportGenerator(
            developer_name=developer_name,
            developer_type=developer_type,
            llm_provider=llm_provider
        )
        self.rag_integration = RAGIntegration(
            developer_name=developer_name,
            developer_type=developer_type,
            logger=logger,
            prompt_manager=self.prompt_manager
        )
        self.llm_wrapper = LLMClientWrapper(
            llm_client=self.llm_client,
            developer_name=developer_name,
            developer_type=developer_type,
            llm_provider=llm_provider,
            llm_model=llm_model,
            logger=logger
        )
        self.tdd_phases = TDDPhases(
            file_manager=self.file_manager,
            test_runner=self.test_runner,
            llm_wrapper=self.llm_wrapper,
            logger=logger,
            retry_coordinator=self.retry_coordinator
        )

        self._log_info(f"‚úÖ {developer_name} initialized with {llm_provider}")

    def execute(
        self,
        task_title: str,
        task_description: str,
        adr_content: str,
        adr_file: str,
        output_dir: Path,
        developer_prompt_file: str,
        card_id: str = "",
        rag_agent = None,
        parsed_requirements: Optional[Dict] = None
    ) -> Dict:
        """
        Execute task using REQUIREMENTS-DRIVEN workflow selection

        Workflow options:
        - TDD: For code (tests first, minimal implementation)
        - QUALITY_DRIVEN: For notebooks/demos (comprehensive output, rich validation)

        Args:
            task_title: Title of task
            task_description: Task description
            adr_content: ADR content
            adr_file: Path to ADR file
            output_dir: Output directory for implementation
            developer_prompt_file: Path to developer prompt file
            card_id: Card ID for querying RAG feedback (optional)
            rag_agent: RAG Agent for querying feedback (optional)
            parsed_requirements: Parsed requirements (optional)

        Returns:
            Dict with implementation results
        """
        self.debug_trace("execute", task_title=task_title, developer=self.developer_name, card_id=card_id)

        # Setup execution context
        context = self._setup_execution_context(
            task_title, task_description, adr_content, output_dir,
            developer_prompt_file, card_id, rag_agent
        )

        # Requirements-driven workflow selection
        strategy = self._select_workflow_strategy(task_title, task_description, parsed_requirements)

        self._log_info(f"üìã {self.developer_name} using {strategy.workflow.value.upper()} workflow")

        try:
            # Execute appropriate workflow
            results = self._execute_workflow(
                strategy, task_title, task_description, adr_content, output_dir, context
            )

            # Generate and save solution report
            solution_report = self.report_generator.finalize_solution_report(results, output_dir)

            self.debug_dump_if_enabled("solution", "Solution Report", {
                "workflow": strategy.workflow.value,
                "num_files": len(results.get('implementation_files', results.get('green', {}).get('implementation_files', [])))
            })

            return solution_report

        except Exception as e:
            self._log_error(f"‚ùå {self.developer_name} workflow failed: {e}")
            raise create_wrapped_exception(
                e,
                DeveloperExecutionError,
                f"Developer {self.developer_name} workflow failed",
                {
                    "developer_name": self.developer_name,
                    "developer_type": self.developer_type,
                    "task_title": task_title,
                    "card_id": card_id
                }
            )

    # ========== Workflow Orchestration ==========

    def _execute_workflow(self, strategy, task_title, task_description, adr_content, output_dir, context):
        """Execute workflow based on strategy - Strategy Pattern"""
        if strategy.workflow == WorkflowType.TDD:
            return self._execute_tdd_workflow(
                task_title, task_description, adr_content, output_dir, context
            )

        return self._execute_quality_driven_workflow(
            task_title, task_description, adr_content, output_dir, context
        )

    def _execute_tdd_workflow(self, task_title, task_description, adr_content, output_dir, context):
        """
        Execute TDD workflow: RED ‚Üí GREEN ‚Üí REFACTOR

        Delegates to TDDPhases orchestrator
        """
        # RED Phase: Generate failing tests
        red_results = self.tdd_phases.execute_red_phase(
            task_title, task_description, adr_content, output_dir, context
        )

        # GREEN Phase: Implement code to pass tests
        green_results = self.tdd_phases.execute_green_phase(
            task_title, task_description, adr_content, output_dir, context, red_results
        )

        # REFACTOR Phase: Improve code quality
        refactor_results = self.tdd_phases.execute_refactor_phase(
            task_title, output_dir, context, green_results
        )

        return {
            'red': {
                'test_files': red_results.files,
                'test_results': red_results.test_results
            },
            'green': {
                'implementation_files': green_results.files,
                'test_results': green_results.test_results
            },
            'refactor': {
                'refactored_files': refactor_results.files,
                'test_results': refactor_results.test_results
            }
        }

    def _execute_quality_driven_workflow(self, task_title, task_description, adr_content, output_dir, context):
        """
        Execute quality-driven workflow: Direct implementation with validation

        For notebooks, demos, presentations - prioritize comprehensive output
        """
        self._log_info("üéØ QUALITY-DRIVEN workflow: Generating comprehensive implementation...")

        # Query RAG for examples
        rag_examples = self.rag_integration.query_rag_for_examples(
            self.rag, task_title, task_description
        ) if hasattr(self, 'rag') else ""

        # Build quality-driven prompt
        prompt = self._build_quality_prompt(
            task_title, task_description, adr_content, context, rag_examples
        )

        # Generate implementation using LLM
        response = self.llm_wrapper.call_llm(prompt)
        implementation = self.llm_wrapper.parse_implementation(response.content)

        # Validate import consistency before writing files
        validation_result = self._validate_import_consistency(implementation)

        if not validation_result['valid']:
            self._log_warning("‚ö†Ô∏è Import consistency issues detected:")
            for issue in validation_result['issues']:
                self._log_warning(f"  {issue}")

            # Log available exports for debugging
            self._log_info("üìã Module exports detected:")
            for module, exports in validation_result['module_exports'].items():
                self._log_info(f"  {module}: {', '.join(exports)}")

            # Note: Still write files but flag for ValidationStage to catch
            self._log_warning("‚ö†Ô∏è Files will be written but may fail validation tests")

        else:
            self._log_info("‚úÖ Import consistency validation passed")

        # Write files
        files_written = self.file_manager.write_implementation_files(implementation, output_dir)

        self._log_info(f"‚úÖ Quality-driven workflow complete: {len(files_written)} files written")

        return {
            'implementation_files': implementation.get('implementation_files', []),
            'test_files': implementation.get('test_files', [])
        }

    # ========== Context Setup ==========

    def _setup_execution_context(self, task_title, task_description, adr_content,
                                  output_dir, developer_prompt_file, card_id, rag_agent):
        """
        Setup execution context by gathering all necessary data

        Returns:
            Dict containing all context data
        """
        self._log_info(f"üöÄ {self.developer_name} starting workflow...")

        # Create output directory
        output_dir.mkdir(parents=True, exist_ok=True)

        # Query KG for similar implementations (if AI service available)
        kg_context = self._query_kg_context(task_title, task_description)

        # Query RAG for code review feedback
        code_review_feedback = None
        if rag_agent and card_id:
            code_review_feedback = self.rag_integration.query_code_review_feedback(rag_agent, card_id)

        # Get developer prompt (from RAG if available, else file-based)
        developer_prompt = self._get_developer_prompt(
            task_title, adr_content, code_review_feedback, rag_agent, developer_prompt_file
        )

        # Load example slides if task involves HTML/slides
        example_slides = None
        if "slide" in task_description.lower() or "html" in task_description.lower():
            example_slides = self.file_manager.load_example_slides(adr_content)

        # Query RAG for refactoring instructions
        refactoring_instructions = None
        if rag_agent:
            language = self._detect_language_from_task(task_description)
            refactoring_instructions = self.rag_integration.query_refactoring_instructions(
                rag_agent, task_title, language
            )

        return {
            'kg_context': kg_context,
            'code_review_feedback': code_review_feedback,
            'developer_prompt': developer_prompt,
            'example_slides': example_slides,
            'refactoring_instructions': refactoring_instructions
        }

    def _select_workflow_strategy(self, task_title, task_description, parsed_requirements):
        """
        Select workflow strategy based on requirements

        Uses RequirementsDrivenValidator if available, else task type detection
        """
        try:
            from requirements_driven_validator import RequirementsDrivenValidator
            validator = RequirementsDrivenValidator(self.logger)
            return validator.analyze_requirements(task_title, task_description, parsed_requirements)
        except ImportError:
            # Fallback: simple strategy based on task type detection
            from agents.developer.models import WorkflowType, TaskType, ExecutionStrategy

            task_type = self._detect_task_type(task_title, task_description)

            # Simple mapping: notebook/presentation ‚Üí quality-driven, code ‚Üí TDD
            workflow = (WorkflowType.QUALITY_DRIVEN
                       if task_type in [TaskType.NOTEBOOK, TaskType.PRESENTATION, TaskType.HTML]
                       else WorkflowType.TDD)

            # Create simple strategy object
            class SimpleStrategy:
                def __init__(self, workflow, artifact_type):
                    self.workflow = workflow
                    self.artifact_type = artifact_type

            return SimpleStrategy(workflow, task_type)

    def _detect_task_type(self, task_title, task_description):
        """Detect task type from title and description"""
        from agents.developer.models import TaskType

        combined = f"{task_title} {task_description}".lower()

        # Guard: notebook
        if any(kw in combined for kw in ['jupyter', 'notebook', 'ipynb']):
            return TaskType.NOTEBOOK

        # Guard: presentation
        if any(kw in combined for kw in ['slide', 'presentation', 'demo']):
            return TaskType.PRESENTATION

        # Guard: HTML
        if any(kw in combined for kw in ['html', 'webpage']):
            return TaskType.HTML

        # Default: code
        return TaskType.CODE

    def _detect_language_from_task(self, task_description: str) -> str:
        """
        Detect programming language from task description

        Args:
            task_description: Task description text

        Returns:
            Detected language string (python, java, javascript, go, rust)
            Defaults to "python" if not detected
        """
        # Normalize to lowercase for case-insensitive matching
        description_lower = task_description.lower()

        # Language detection keywords with priority order
        language_keywords = {
            'java': ['java', 'spring', 'maven', 'gradle', 'springboot', 'spring boot'],
            'javascript': ['javascript', 'typescript', 'node', 'react', 'nodejs', 'node.js',
                          'angular', 'vue', 'npm', 'yarn', 'jest', 'webpack'],
            'go': ['golang', 'go '],  # 'go ' with space to avoid matching 'django', etc.
            'rust': ['rust', 'cargo'],
            'python': ['python', 'django', 'flask', 'fastapi', 'pytest', 'pip']
        }

        # Check for language keywords
        for language, keywords in language_keywords.items():
            if any(keyword in description_lower for keyword in keywords):
                return language

        # Default to python if no language detected
        return "python"

    # ========== Helper Methods (Delegation) ==========

    def _query_kg_context(self, task_title, task_description):
        """Query KG for similar implementations"""
        # Guard: no AI service
        if not self.ai_service:
            return None

        try:
            from ai_query_service import QueryType
            result = self.ai_service.query(
                query_type=QueryType.SIMILAR_IMPLEMENTATIONS,
                query_text=f"{task_title}: {task_description}",
                context={"task_title": task_title}
            )
            return result.data if result.success else None
        except Exception as e:
            self._log_warning(f"KG query failed: {e}")
            return None

    def _get_developer_prompt(self, task_title, adr_content, code_review_feedback, rag_agent, prompt_file):
        """Get developer prompt (RAG or file-based)"""
        # Try RAG first
        if self.prompt_manager and rag_agent:
            return self.rag_integration.get_developer_prompt_from_rag(
                task_title, adr_content, code_review_feedback, rag_agent
            )

        # Fallback: read from file
        return self.file_manager.read_developer_prompt(prompt_file)

    def _build_quality_prompt(self, task_title, task_description, adr_content, context, rag_examples):
        """Build prompt for quality-driven workflow"""
        return f"""
Task: {task_title}
Description: {task_description}

Architecture Decision:
{adr_content}

{context.get('developer_prompt', '')}

{rag_examples}

Generate a comprehensive, high-quality implementation following all guidelines.

CRITICAL CONSISTENCY REQUIREMENTS:
1. **Import Consistency**: Every imported function/class MUST be defined in the imported module
   - Before importing from a module, ensure that module exports those exact names
   - Example: If you write "from database import get_user, validate_password"
            then database/__init__.py MUST define both get_user() and validate_password()

2. **Complete Implementation**: NO placeholder functions or missing implementations
   - All functions must be fully implemented with working logic
   - All helper functions must be defined before use
   - All dependencies must be included in the generated files

3. **Self-Contained Code**: Generated code must work without external dependencies
   - If a function is called, it must be defined somewhere in your generated files
   - If a module is imported, it must be in your generated files or Python stdlib

4. **Test Compatibility**: Tests must import only what implementation provides
   - Review all test imports and ensure implementation files export those names
   - Ensure test function calls match implementation function signatures

LOGIC CORRECTNESS REQUIREMENTS:
5. **Edge Case Handling**: Handle ALL edge cases explicitly
   - Empty inputs (empty strings, empty lists, None values)
   - Boundary conditions (zero, negative numbers, maximum values)
   - Invalid data types or formats
   - Example: Always check if user is None before accessing user properties

6. **Error Handling**: Implement comprehensive error handling
   - Use try/except blocks for operations that can fail (I/O, parsing, external calls)
   - Provide meaningful error messages
   - Don't silently fail - log or raise exceptions appropriately
   - Handle timeouts and resource exhaustion

7. **Logic Flow**: Ensure correct control flow
   - Early returns for invalid inputs (guard clauses)
   - Proper boolean logic (avoid double negatives)
   - Clear conditional branches
   - No unreachable code

8. **Data Validation**: Validate all inputs
   - Type checking where appropriate
   - Range validation for numbers
   - Format validation for strings (email, phone, etc.)
   - Length limits for collections

9. **Test Quality**: Write comprehensive, meaningful tests
   - Happy path: Test normal/expected behavior
   - Error cases: Test error handling
   - Edge cases: Test boundary conditions
   - Integration: Test components working together
   - Aim for >80% code coverage

VALIDATION CHECKLIST (verify before returning):
‚òê All imports in implementation files have matching exports
‚òê All imports in test files have matching exports in implementation
‚òê No undefined functions are called
‚òê No placeholder comments (TODO, FIXME, etc.)
‚òê All async functions properly use await for async calls
‚òê All error cases are handled with try/except or validation
‚òê Edge cases (None, empty, invalid) are explicitly handled
‚òê Input validation is present for all public functions
‚òê Tests cover happy path, error cases, and edge cases
‚òê Error messages are clear and actionable

IMPORTANT: Return your response as a JSON object with the following structure:
{{
  "implementation_files": [
    {{
      "path": "relative/path/to/file.py",
      "content": "file content here..."
    }}
  ],
  "test_files": [
    {{
      "path": "tests/test_file.py",
      "content": "test content here..."
    }}
  ]
}}

Include all necessary files to complete the task. Each file must have a "path" and "content" field.
"""

    def _validate_import_consistency(self, generated_code: Dict) -> Dict:
        """
        Validate that all imports have matching exports

        WHY: Catches import inconsistencies before files are written,
             preventing ValidationStage failures due to missing functions.

        IMPROVEMENTS:
        - Handles relative imports (from .module import X)
        - Resolves __init__.py re-exports
        - Better distinguishes between callables and variables

        Args:
            generated_code: Dict with 'implementation_files' and 'test_files'

        Returns:
            Dict with validation results and issues found
        """
        import ast
        from pathlib import Path

        issues = []
        all_files = generated_code.get('implementation_files', []) + generated_code.get('test_files', [])

        # Build comprehensive file and export maps
        file_map = {}  # path -> (module_name, content, tree)
        module_exports = {}  # module_name -> set of exports

        # First pass: Parse all files and build file map
        for file_dict in all_files:
            path = file_dict.get('path', '')
            content = file_dict.get('content', '')

            # Determine module name from path
            path_obj = Path(path)

            # Skip test files for export tracking (tests don't export)
            if path.startswith('tests/'):
                module_name = None
            elif path_obj.name == '__init__.py':
                # database/__init__.py -> module is 'database'
                module_name = str(path_obj.parent) if path_obj.parent != Path('.') else None
            elif path_obj.suffix == '.py':
                # database/models.py -> module is 'database.models'
                parts = path_obj.with_suffix('').parts
                module_name = '.'.join(parts)
            else:
                module_name = None

            try:
                tree = ast.parse(content)
                file_map[path] = (module_name, content, tree)
            except SyntaxError:
                issues.append(f"‚ö†Ô∏è Syntax error in {path} - cannot validate imports")
                file_map[path] = (module_name, content, None)
                continue

        # Second pass: Extract exports from each module
        for path, (module_name, content, tree) in file_map.items():
            if not module_name or not tree:
                continue

            exports = set()

            # Walk AST and collect only module-level definitions
            for node in tree.body:  # Only top-level, not nested
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    exports.add(node.name)
                elif isinstance(node, ast.ClassDef):
                    exports.add(node.name)
                elif isinstance(node, ast.Assign):
                    # Only track special variables like __all__
                    for target in node.targets:
                        if isinstance(target, ast.Name) and target.id == '__all__':
                            exports.add('__all__')
                elif isinstance(node, ast.ImportFrom):
                    # Track re-exports in __init__.py
                    if path.endswith('__init__.py'):
                        for alias in node.names:
                            if alias.name != '*':
                                exports.add(alias.asname if alias.asname else alias.name)

            # Get base module name (e.g., 'database' from 'database.models')
            base_module = module_name.split('.')[0]

            # Add exports to both specific module and base package
            if module_name not in module_exports:
                module_exports[module_name] = set()
            module_exports[module_name].update(exports)

            # Also add to base module if this is a submodule
            if '.' in module_name:
                if base_module not in module_exports:
                    module_exports[base_module] = set()
                module_exports[base_module].update(exports)

        # Third pass: Validate imports
        for path, (module_name, content, tree) in file_map.items():
            if not tree:
                continue

            # Get directory of current file for resolving relative imports
            current_dir = str(Path(path).parent) if Path(path).parent != Path('.') else ''

            for node in tree.body:
                if isinstance(node, ast.ImportFrom):
                    import_module = node.module

                    # Resolve relative imports
                    if node.level > 0:  # Relative import (from .module or from ..module)
                        if node.level == 1 and import_module:
                            # from .database import X -> resolve to current package
                            resolved_module = f"{current_dir}.{import_module}" if current_dir else import_module
                        elif node.level == 1 and not import_module:
                            # from . import X -> resolve to current package
                            resolved_module = current_dir if current_dir else None
                        else:
                            # from .. import X -> resolve to parent package (simplified)
                            parent = str(Path(current_dir).parent) if current_dir else None
                            resolved_module = f"{parent}.{import_module}" if parent and import_module else parent
                    else:
                        resolved_module = import_module

                    # Check if module exists and has the requested exports
                    if resolved_module and resolved_module in module_exports:
                        for alias in node.names:
                            name = alias.name
                            if name != '*' and name not in module_exports[resolved_module]:
                                # Also check base module (e.g., 'database' when importing from 'database.models')
                                base_module = resolved_module.split('.')[0]
                                if base_module not in module_exports or name not in module_exports[base_module]:
                                    available = sorted(module_exports[resolved_module])
                                    issues.append(
                                        f"‚ùå {path}: imports '{name}' from '{import_module or '(relative)'}' "
                                        f"but '{resolved_module}' doesn't export '{name}'\n"
                                        f"   Available in '{resolved_module}': {', '.join(available) if available else 'none'}"
                                    )

        return {
            'valid': len(issues) == 0,
            'issues': issues,
            'module_exports': {k: sorted(list(v)) for k, v in module_exports.items()}
        }

    # ========== Initialization Helpers ==========

    def _create_llm_client(self):
        """Create LLM client with error handling"""
        try:
            llm_client = create_llm_client(self.llm_provider)
            return llm_client
        except Exception as e:
            self._log_error(f"‚ùå Failed to initialize LLM client: {e}")
            raise create_wrapped_exception(
                e,
                LLMClientError,
                f"Failed to initialize LLM client for {self.developer_name}",
                {"developer_name": self.developer_name, "llm_provider": self.llm_provider}
            )

    def _initialize_prompt_manager(self, rag_agent):
        """Initialize PromptManager with early returns"""
        # Guard: not available
        if not PROMPT_MANAGER_AVAILABLE:
            return None

        # Guard: no RAG agent
        if not rag_agent:
            return None

        try:
            manager = PromptManager(rag_agent, verbose=False)
            self._log_info("‚úÖ Prompt manager initialized (RAG-based prompts)")
            return manager
        except Exception as e:
            self._log_warning(f"‚ö†Ô∏è  Could not initialize prompt manager: {e}")
            return None

    def _initialize_ai_service(self, ai_service, rag_agent):
        """Initialize AI Query Service with early returns"""
        # Guard: not available
        if not AI_QUERY_SERVICE_AVAILABLE:
            return None

        # Guard: already provided
        if ai_service:
            self._log_info("‚úÖ Using provided AI Query Service")
            return ai_service

        try:
            service = create_ai_query_service(
                llm_client=self.llm_client,
                rag=rag_agent,
                logger=self.logger,
                verbose=False
            )
            self._log_info("‚úÖ AI Query Service initialized (KG-First enabled)")
            return service
        except Exception as e:
            self._log_warning(f"‚ö†Ô∏è  Could not initialize AI Query Service: {e}")
            return None

    def _initialize_retry_coordinator(self):
        """Initialize Retry Coordinator with early returns"""
        # Guard: not available
        if not RETRY_COORDINATOR_AVAILABLE:
            return None

        try:
            max_retries = int(os.getenv("ARTEMIS_MAX_VALIDATION_RETRIES", "3"))
            acceptance_threshold = float(os.getenv("ARTEMIS_CONFIDENCE_THRESHOLD", "0.85"))

            coordinator = RetryCoordinator(
                logger=self.logger,
                max_retries=max_retries,
                acceptance_threshold=acceptance_threshold
            )
            self._log_info(f"‚úÖ Retry Coordinator initialized (max={max_retries}, threshold={acceptance_threshold:.2f})")
            return coordinator
        except Exception as e:
            self._log_warning(f"‚ö†Ô∏è  Could not initialize Retry Coordinator: {e}")
            return None

    # ========== Logging Helpers ==========

    def _log_info(self, message: str):
        """Log info message"""
        if self.logger:
            self.logger.log(message, "INFO")

    def _log_warning(self, message: str):
        """Log warning message"""
        if self.logger:
            self.logger.log(message, "WARNING")

    def _log_error(self, message: str):
        """Log error message"""
        if self.logger:
            self.logger.log(message, "ERROR")


__all__ = [
    "Developer"
]

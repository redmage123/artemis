#!/usr/bin/env python3
"""
Validation Pipeline - Continuous validation during code generation

This module provides multi-stage validation that catches hallucinations and errors
DURING the generation process, not just at the end.

Layers:
1. Preflight (before) - Static checks
2. Strategy Selection - Choose workflow
3. Pipeline (during) - Continuous validation ← THIS MODULE
4. Quality Gates (after) - Final validation

Integration with existing validators:
- Uses PreflightValidator for syntax checks
- Works with RequirementsDrivenValidator for strategy
- Complements ArtifactQualityValidator for final checks
"""

from typing import Dict, List, Optional, Tuple, Callable
from dataclasses import dataclass
from pathlib import Path
from enum import Enum
import ast
import re
import tempfile


class ValidationStage(Enum):
    """Stages of code generation that require validation"""
    IMPORTS = "imports"
    SIGNATURE = "signature"
    DOCSTRING = "docstring"
    BODY = "body"
    TESTS = "tests"
    FULL_CODE = "full_code"


@dataclass
class StageValidationResult:
    """Result of validating a single stage"""
    stage: ValidationStage
    passed: bool
    checks: Dict[str, bool]  # check_name -> passed
    feedback: List[str]  # Human-readable feedback
    severity: str  # "critical", "high", "medium", "low"
    suggestion: Optional[str] = None

    def __str__(self):
        status = "✅" if self.passed else "❌"
        failed = [k for k, v in self.checks.items() if not v]
        return f"{status} {self.stage.value}: {len(failed)} issues" if failed else f"{status} {self.stage.value}"


class ValidationPipeline:
    """
    Continuous validation pipeline for code generation.

    Validates code at each stage of generation, providing immediate feedback
    to the LLM to prevent hallucinations from propagating.

    Usage:
        pipeline = ValidationPipeline(llm_client=llm, logger=logger)

        # Validate incrementally
        result = pipeline.validate_stage(code, ValidationStage.IMPORTS)
        if not result.passed:
            # Get feedback and regenerate
            feedback = pipeline.get_regeneration_prompt(result)
    """

    def __init__(self, llm_client=None, logger=None, strict_mode: bool = True):
        """
        Args:
            llm_client: LLM client for regeneration (optional)
            logger: ArtemisLogger instance
            strict_mode: If True, fail on any validation error. If False, allow warnings.
        """
        self.llm_client = llm_client
        self.logger = logger
        self.strict_mode = strict_mode
        self.validation_history: List[StageValidationResult] = []

    def _dispatch_validation(self, code: str, stage: ValidationStage, context: Optional[Dict]) -> Tuple[Dict, List, str]:
        """Dispatch validation to appropriate stage handler"""
        if stage == ValidationStage.IMPORTS:
            return self._validate_imports(code, context)
        if stage == ValidationStage.SIGNATURE:
            return self._validate_signature(code, context)
        if stage == ValidationStage.DOCSTRING:
            return self._validate_docstring(code, context)
        if stage == ValidationStage.BODY:
            return self._validate_body(code, context)
        if stage == ValidationStage.TESTS:
            return self._validate_tests(code, context)
        if stage == ValidationStage.FULL_CODE:
            return self._validate_full_code(code, context)
        return {}, [], "medium"

    def validate_stage(self, code: str, stage: ValidationStage, context: Optional[Dict] = None) -> StageValidationResult:
        """
        Validate code at a specific generation stage.

        Args:
            code: Current generated code
            stage: Which stage we're validating
            context: Additional context (task requirements, imports needed, etc.)

        Returns:
            StageValidationResult with pass/fail and specific feedback
        """
        checks, feedback, severity = self._dispatch_validation(code, stage, context)

        # Determine pass/fail
        critical_checks = [k for k, v in checks.items() if not v and severity == "critical"]
        high_checks = [k for k, v in checks.items() if not v and severity == "high"]

        if self.strict_mode:
            passed = all(checks.values())
        else:
            # Only fail on critical issues in non-strict mode
            passed = len(critical_checks) == 0

        result = StageValidationResult(
            stage=stage,
            passed=passed,
            checks=checks,
            feedback=feedback,
            severity=severity,
            suggestion=self._generate_suggestion(stage, feedback) if not passed else None
        )

        self.validation_history.append(result)

        if not self.logger:
            return result

        if result.passed:
            self.logger.log(f"✅ {stage.value} validation passed", "INFO")
        else:
            self.logger.log(f"❌ {stage.value} validation failed: {', '.join(feedback)}", "WARNING")

        return result

    def _check_required_imports(self, code: str, required: List[str], checks: Dict, feedback: List):
        """Helper to check for required imports"""
        for req in required:
            has_req = req in code
            checks[f'has_{req}'] = has_req
            if not has_req:
                feedback.append(f"Missing required import: {req}")

    def _check_has_imports(self, code: str, code_lines: List[str], checks: Dict, feedback: List):
        """Helper to check if code has imports"""
        if len(code_lines) <= 5:
            return  # Trivial code
        has_imports = any('import ' in line for line in code)
        checks['has_imports'] = has_imports
        if not has_imports:
            feedback.append("Code has no import statements - likely missing dependencies")

    def _check_expected_methods(self, code: str, expected_methods: List[str], checks: Dict, feedback: List):
        """Helper to check for expected methods"""
        for method in expected_methods:
            has_method = method in code
            checks[f'uses_{method}'] = has_method
            if not has_method:
                feedback.append(f"Expected to use method: {method}")

    def _validate_imports(self, code: str, context: Optional[Dict]) -> Tuple[Dict, List, str]:
        """Validate import statements"""
        checks = {}
        feedback = []

        # Check 1: Has imports (if code is non-trivial)
        lines = [l.strip() for l in code.split('\n') if l.strip()]
        code_lines = [l for l in lines if not l.startswith('#')]
        self._check_has_imports(code, code_lines, checks, feedback)

        # Check 2: No import errors (try to parse)
        try:
            tree = ast.parse(code)
            import_nodes = [n for n in ast.walk(tree) if isinstance(n, (ast.Import, ast.ImportFrom))]
            checks['imports_parseable'] = True

            # Check 3: No star imports (bad practice)
            star_imports = [n for n in import_nodes if isinstance(n, ast.ImportFrom) and any(alias.name == '*' for alias in n.names)]
            checks['no_star_imports'] = len(star_imports) == 0
            if star_imports:
                feedback.append("Avoid star imports (from X import *) - import specific names")

        except SyntaxError as e:
            checks['imports_parseable'] = False
            feedback.append(f"Import syntax error: {e.msg}")

        # Check 4: Known bad imports
        bad_imports = ['from os import system', 'import pickle', '__import__']
        for bad in bad_imports:
            if bad in code:
                checks[f'no_dangerous_import_{bad}'] = False
                feedback.append(f"Dangerous import detected: {bad}")

        # Check 5: Verify imports exist (if context provided)
        if context and 'required_imports' in context:
            self._check_required_imports(code, context['required_imports'], checks, feedback)

        severity = "critical" if not checks.get('imports_parseable', True) else "high"

        return checks, feedback, severity

    def _check_function_signature(self, func, checks: Dict, feedback: List):
        """Helper to check a single function signature"""
        # Check 1: Has docstring
        has_docstring = (ast.get_docstring(func) is not None)
        checks[f'{func.name}_has_docstring'] = has_docstring
        if not has_docstring:
            feedback.append(f"Function '{func.name}' missing docstring")

        # Check 2: Has type hints
        has_return_hint = func.returns is not None
        checks[f'{func.name}_has_return_hint'] = has_return_hint
        if not has_return_hint and func.name != '__init__':
            feedback.append(f"Function '{func.name}' missing return type hint")

        # Check 3: Parameters have type hints
        args_with_hints = [arg for arg in func.args.args if arg.annotation is not None]
        non_self_args = [arg for arg in func.args.args if arg.arg not in ['self', 'cls']]
        if not non_self_args:
            return
        param_hint_ratio = len(args_with_hints) / len(func.args.args)
        checks[f'{func.name}_params_typed'] = param_hint_ratio > 0.5
        if param_hint_ratio < 0.5:
            feedback.append(f"Function '{func.name}' parameters need type hints")

    def _validate_signature(self, code: str, context: Optional[Dict]) -> Tuple[Dict, List, str]:
        """Validate function/class signatures"""
        checks = {}
        feedback = []

        try:
            tree = ast.parse(code)

            # Find function definitions
            functions = [n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]

            for func in functions:
                self._check_function_signature(func, checks, feedback)

            # Find class definitions
            classes = [n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]

            for cls in classes:
                # Check: Has docstring
                has_docstring = (ast.get_docstring(cls) is not None)
                checks[f'{cls.name}_has_docstring'] = has_docstring
                if not has_docstring:
                    feedback.append(f"Class '{cls.name}' missing docstring")

            checks['parseable'] = True

        except SyntaxError as e:
            checks['parseable'] = False
            feedback.append(f"Signature syntax error: {e.msg}")

        severity = "critical" if not checks.get('parseable', True) else "medium"

        return checks, feedback, severity

    def _validate_docstring(self, code: str, context: Optional[Dict]) -> Tuple[Dict, List, str]:
        """Validate docstrings"""
        checks = {}
        feedback = []

        try:
            tree = ast.parse(code)

            # Check module docstring
            module_docstring = ast.get_docstring(tree)
            checks['has_module_docstring'] = module_docstring is not None
            if not module_docstring:
                feedback.append("Missing module-level docstring")

            # Check function docstrings
            functions = [n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]
            for func in functions:
                # Skip private/dunder methods
                if func.name.startswith('_') and not func.name.startswith('__'):
                    continue

                docstring = ast.get_docstring(func)
                checks[f'{func.name}_docstring'] = docstring is not None

                if not docstring:
                    feedback.append(f"Function '{func.name}' missing docstring")
                    continue

                # Check docstring quality
                has_args_section = 'Args:' in docstring or 'Parameters:' in docstring
                has_returns_section = 'Returns:' in docstring or 'Return:' in docstring

                if func.args.args and not has_args_section:
                    checks[f'{func.name}_docstring_has_args'] = False
                    feedback.append(f"Docstring for '{func.name}' missing Args section")

                if func.returns and not has_returns_section and func.name != '__init__':
                    checks[f'{func.name}_docstring_has_returns'] = False
                    feedback.append(f"Docstring for '{func.name}' missing Returns section")

        except SyntaxError as e:
            checks['parseable'] = False
            feedback.append(f"Syntax error when checking docstrings: {e.msg}")

        severity = "low"  # Docstrings are important but not critical

        return checks, feedback, severity

    def _validate_body(self, code: str, context: Optional[Dict]) -> Tuple[Dict, List, str]:
        """Validate function/class body"""
        checks = {}
        feedback = []

        # Check 1: No placeholders (CRITICAL for hallucination prevention)
        placeholders = ['TODO', 'FIXME', 'XXX', '...', 'pass  # implementation', 'raise NotImplementedError']
        for placeholder in placeholders:
            if placeholder in code:
                checks[f'no_{placeholder.lower().replace(" ", "_")}'] = False
                feedback.append(f"Found placeholder: '{placeholder}' - code must be complete")

        # Check 2: No obvious errors
        error_patterns = [
            (r'\.save\(\)', "SQLAlchemy uses db.session.add() and commit(), not .save()"),
            (r'User\.create\(', "SQLAlchemy uses User() constructor + db.session.add(), not .create()"),
            (r'\.find\(\)', "SQLAlchemy uses .query().filter(), not .find()"),
        ]

        for pattern, message in error_patterns:
            if re.search(pattern, code):
                checks[f'no_error_{pattern[:20]}'] = False
                feedback.append(message)

        # Check 3: Try to parse and compile
        try:
            tree = ast.parse(code)
            compile(code, '<string>', 'exec')
            checks['syntax_valid'] = True
            checks['compiles'] = True

            # Check 4: No bare excepts
            for node in ast.walk(tree):
                if not isinstance(node, ast.ExceptHandler):
                    continue
                if node.type is None:
                    checks['no_bare_except'] = False
                    feedback.append("Avoid bare 'except:' - catch specific exceptions")
                    break

        except SyntaxError as e:
            checks['syntax_valid'] = False
            checks['compiles'] = False
            feedback.append(f"Syntax error at line {e.lineno}: {e.msg}")
        except Exception as e:
            checks['compiles'] = False
            feedback.append(f"Compilation error: {str(e)}")

        # Check 5: No obviously wrong method calls (if context provided)
        if context and 'expected_methods' in context:
            self._check_expected_methods(code, context['expected_methods'], checks, feedback)

        severity = "critical" if not checks.get('compiles', True) else "high"

        return checks, feedback, severity

    def _validate_tests(self, code: str, context: Optional[Dict]) -> Tuple[Dict, List, str]:
        """Validate test code"""
        checks = {}
        feedback = []

        # Check 1: Has test functions
        has_test_functions = bool(re.search(r'def test_\w+\(', code))
        checks['has_test_functions'] = has_test_functions
        if not has_test_functions:
            feedback.append("No test functions found (must start with 'test_')")

        # Check 2: Has assertions
        has_assertions = bool(re.search(r'assert\s+', code))
        checks['has_assertions'] = has_assertions
        if not has_assertions:
            feedback.append("No assertions found in tests")

        # Check 3: Imports pytest or unittest
        imports_test_framework = ('import pytest' in code or 'import unittest' in code)
        checks['imports_test_framework'] = imports_test_framework
        if not imports_test_framework:
            feedback.append("Missing test framework import (pytest or unittest)")

        # Check 4: Try to parse
        try:
            ast.parse(code)
            checks['parseable'] = True
        except SyntaxError as e:
            checks['parseable'] = False
            feedback.append(f"Test syntax error: {e.msg}")

        severity = "high"

        return checks, feedback, severity

    def _validate_full_code(self, code: str, context: Optional[Dict]) -> Tuple[Dict, List, str]:
        """Validate complete code artifact"""
        checks = {}
        feedback = []

        # Run all previous validations
        import_checks, import_feedback, _ = self._validate_imports(code, context)
        sig_checks, sig_feedback, _ = self._validate_signature(code, context)
        body_checks, body_feedback, _ = self._validate_body(code, context)

        checks.update(import_checks)
        checks.update(sig_checks)
        checks.update(body_checks)

        feedback.extend(import_feedback)
        feedback.extend(sig_feedback)
        feedback.extend(body_feedback)

        # Additional full-code checks

        # Check: File size reasonable (not too small/large)
        line_count = len([l for l in code.split('\n') if l.strip()])
        checks['reasonable_size'] = 5 <= line_count <= 1000
        if line_count < 5:
            feedback.append("Code too short - likely incomplete")
        if line_count > 1000:
            feedback.append("Code very long - consider splitting into modules")

        # Check: Has proper structure
        try:
            tree = ast.parse(code)
            has_structure = any(isinstance(n, (ast.FunctionDef, ast.ClassDef)) for n in tree.body)
            checks['has_structure'] = has_structure
            if not has_structure:
                feedback.append("Code has no functions or classes - likely incomplete")
        except:
            pass

        # Determine overall severity
        if not checks.get('compiles', True) or not checks.get('parseable', True):
            severity = "critical"
        else:
            has_placeholder_issues = any('placeholder' in k for k, v in checks.items() if not v)
            severity = "critical" if has_placeholder_issues else "medium"

        return checks, feedback, severity

    def _generate_suggestion(self, stage: ValidationStage, feedback: List[str]) -> str:
        """Generate actionable suggestion for fixing issues"""
        if not feedback:
            return None

        suggestions = {
            ValidationStage.IMPORTS: "Add missing imports at the top of the file. Import specific names, not '*'.",
            ValidationStage.SIGNATURE: "Add type hints and docstrings to all functions and classes.",
            ValidationStage.DOCSTRING: "Add comprehensive docstrings with Args and Returns sections.",
            ValidationStage.BODY: "Complete the implementation - remove all TODOs and placeholders. Verify method calls are correct for the framework being used.",
            ValidationStage.TESTS: "Write actual test functions with assertions. Import pytest or unittest.",
            ValidationStage.FULL_CODE: "Fix all validation errors before proceeding."
        }

        base_suggestion = suggestions.get(stage, "Fix the validation errors.")
        return f"{base_suggestion}\n\nSpecific issues:\n" + "\n".join(f"- {f}" for f in feedback[:3])

    def get_regeneration_prompt(self, result: StageValidationResult) -> str:
        """
        Generate a prompt for the LLM to regenerate code with fixes.

        Args:
            result: Failed validation result

        Returns:
            Prompt string to send to LLM
        """
        prompt = f"""The {result.stage.value} validation failed with the following issues:

{chr(10).join(f"- {f}" for f in result.feedback)}

{result.suggestion if result.suggestion else ''}

Please regenerate the code to fix these issues. Return ONLY the corrected code, no explanations."""

        return prompt

    def _log_max_retries(self, stage: ValidationStage):
        """Helper to log max retries exceeded"""
        if self.logger:
            self.logger.log(f"❌ Max retries exceeded for {stage.value}", "ERROR")

    def generate_with_validation(self,
                                task: str,
                                stages: List[ValidationStage],
                                max_retries: int = 2) -> Tuple[str, bool]:
        """
        Generate code with continuous validation at each stage.

        Args:
            task: Task description for the LLM
            stages: Stages to validate (in order)
            max_retries: Max retries per stage

        Returns:
            (generated_code, success)
        """
        if not self.llm_client:
            raise ValueError("LLM client required for generation")

        code = ""

        for stage in stages:
            stage_prompt = self._create_stage_prompt(task, stage, code)

            for attempt in range(max_retries + 1):
                # Generate code for this stage
                if self.logger:
                    self.logger.log(f"🔄 Generating {stage.value} (attempt {attempt + 1})", "INFO")

                new_code = self.llm_client.query(stage_prompt)
                combined_code = code + "\n" + new_code if code else new_code

                # Validate
                result = self.validate_stage(combined_code, stage)

                if result.passed:
                    code = combined_code
                    break

                # Handle validation failure
                if attempt >= max_retries:
                    self._log_max_retries(stage)
                    return code, False

                # Regenerate with feedback
                feedback_prompt = self.get_regeneration_prompt(result)
                stage_prompt += f"\n\n{feedback_prompt}"
                if self.logger:
                    self.logger.log(f"⚠️  Validation failed, retrying with feedback", "WARNING")

        return code, True

    def _create_stage_prompt(self, task: str, stage: ValidationStage, existing_code: str) -> str:
        """Create a prompt for generating a specific stage"""
        prompts = {
            ValidationStage.IMPORTS: f"Generate import statements for: {task}\n\nImport only what's needed. No star imports.",
            ValidationStage.SIGNATURE: f"Generate function/class signatures for: {task}\n\nInclude type hints and docstrings.\n\n{existing_code}",
            ValidationStage.BODY: f"Generate the implementation for: {task}\n\nComplete implementation, no TODOs or placeholders.\n\n{existing_code}",
            ValidationStage.TESTS: f"Generate pytest tests for: {task}\n\nInclude multiple test cases with assertions.\n\n{existing_code}",
        }

        return prompts.get(stage, f"Generate {stage.value} for: {task}")

    def get_validation_summary(self) -> Dict:
        """Get summary of all validations performed"""
        total = len(self.validation_history)
        passed = sum(1 for r in self.validation_history if r.passed)
        failed = total - passed

        by_stage = {}
        for result in self.validation_history:
            stage_name = result.stage.value
            if stage_name not in by_stage:
                by_stage[stage_name] = {'passed': 0, 'failed': 0}

            if result.passed:
                by_stage[stage_name]['passed'] += 1
            else:
                by_stage[stage_name]['failed'] += 1

        return {
            'total_validations': total,
            'passed': passed,
            'failed': failed,
            'pass_rate': passed / total if total > 0 else 0.0,
            'by_stage': by_stage,
            'history': self.validation_history
        }

    def reset(self):
        """Clear validation history"""
        self.validation_history = []


# Convenience functions

def validate_python_code(code: str, strict: bool = True) -> StageValidationResult:
    """
    Quick validation of Python code (full validation).

    Args:
        code: Python code to validate
        strict: If True, fail on any issue

    Returns:
        Validation result
    """
    pipeline = ValidationPipeline(strict_mode=strict)
    return pipeline.validate_stage(code, ValidationStage.FULL_CODE)


def validate_incrementally(code_segments: List[Tuple[str, ValidationStage]],
                          strict: bool = True) -> List[StageValidationResult]:
    """
    Validate code incrementally through multiple stages.

    Args:
        code_segments: List of (code, stage) tuples
        strict: If True, fail on any issue

    Returns:
        List of validation results
    """
    pipeline = ValidationPipeline(strict_mode=strict)
    results = []

    for code, stage in code_segments:
        result = pipeline.validate_stage(code, stage)
        results.append(result)

        if not result.passed and strict:
            break

    return results

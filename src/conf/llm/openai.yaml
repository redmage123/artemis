# OpenAI LLM Configuration
provider: openai
model: gpt-4o  # Can override with llm.model=gpt-4o-mini or llm.model=gpt-4-turbo
api_key: ${oc.env:OPENAI_API_KEY}  # Read from environment
max_tokens_per_request: 8000
temperature: 0.7
cost_limit_usd: null  # No limit by default

# Supervisor-specific LLM settings for auto-fix
supervisor:
  model: gpt-4o  # Use GPT-4o for intelligent code fixes
  temperature: 0.3  # Lower temperature for consistent fixes
  max_tokens: 4000
